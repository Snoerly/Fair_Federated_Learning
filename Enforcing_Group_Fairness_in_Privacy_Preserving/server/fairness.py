"""server/fairness.py

Global fairness metrics for GFL.

Implements the paper's server-side group fairness definitions:
- Equal Opportunity Difference (EOD)
- Demographic Parity Difference (DPD)

These are computed on a (global) dataset, which in GFL is the
approximate global dataset generated by AGDG + LSH-CADC.

Assumptions:
- Binary classification labels y in {0,1}
- Binary sensitive group g in {0,1}
- Model outputs can be either probabilities or hard predictions.

We provide:
- metrics_from_scores: works with predicted probabilities for y=1
- metrics_from_preds: works with hard 0/1 predictions
"""

from __future__ import annotations

import numpy as np
from dataclasses import dataclass
from typing import Optional, Tuple


@dataclass
class FairnessMetrics:
    eod: float
    dpd: float


def _safe_mean(x: np.ndarray) -> float:
    if x.size == 0:
        return 0.0
    return float(x.mean())


def metrics_from_scores(
    y_score: np.ndarray,
    y_true: np.ndarray,
    g: np.ndarray,
    threshold: float = 0.5,
) -> FairnessMetrics:
    """Compute (EOD, DPD) from predicted probabilities/scores.

    Parameters
    ----------
    y_score : np.ndarray
        Predicted probability/score for positive class, shape [N].
    y_true : np.ndarray
        True labels in {0,1}, shape [N].
    g : np.ndarray
        Sensitive group in {0,1}, shape [N].
    threshold : float
        Threshold to obtain hard prediction for DPD on scores.

    Notes
    -----
    We follow the paper-style definitions but use score expectations:
    - DPD: |E[p_hat | g=1] - E[p_hat | g=0]|
    - EOD: |E[p_hat | y=1,g=1] - E[p_hat | y=1,g=0]|

    This is smoother and often used for stable tracking.
    """
    y_score = np.asarray(y_score).reshape(-1)
    y_true = np.asarray(y_true).reshape(-1)
    g = np.asarray(g).reshape(-1)

    mask_g0 = (g == 0)
    mask_g1 = (g == 1)

    # Demographic Parity Difference
    dp_g0 = _safe_mean(y_score[mask_g0])
    dp_g1 = _safe_mean(y_score[mask_g1])
    dpd = abs(dp_g1 - dp_g0)

    # Equal Opportunity Difference: conditioned on y=1
    pos = (y_true == 1)
    eod_g0 = _safe_mean(y_score[mask_g0 & pos])
    eod_g1 = _safe_mean(y_score[mask_g1 & pos])
    eod = abs(eod_g1 - eod_g0)

    return FairnessMetrics(eod=eod, dpd=dpd)


def metrics_from_preds(
    y_pred: np.ndarray,
    y_true: np.ndarray,
    g: np.ndarray,
) -> FairnessMetrics:
    """Compute (EOD, DPD) from hard predictions.

    DPD: |P(y_hat=1|g=1) - P(y_hat=1|g=0)|
    EOD: |P(y_hat=1|y=1,g=1) - P(y_hat=1|y=1,g=0)|
    """
    y_pred = np.asarray(y_pred).reshape(-1)
    y_true = np.asarray(y_true).reshape(-1)
    g = np.asarray(g).reshape(-1)

    mask_g0 = (g == 0)
    mask_g1 = (g == 1)

    # DPD
    dp_g0 = _safe_mean(y_pred[mask_g0])
    dp_g1 = _safe_mean(y_pred[mask_g1])
    dpd = abs(dp_g1 - dp_g0)

    # EOD
    pos = (y_true == 1)
    eod_g0 = _safe_mean(y_pred[mask_g0 & pos])
    eod_g1 = _safe_mean(y_pred[mask_g1 & pos])
    eod = abs(eod_g1 - eod_g0)

    return FairnessMetrics(eod=eod, dpd=dpd)


def client_fairness_bias(
    metrics: FairnessMetrics,
    mode: str = "sum",
) -> float:
    """Convert (EOD, DPD) to a single scalar fairness bias Fi.

    The paper denotes a client fairness bias Fi used for weighting.
    We need a scalar; common choices:
    - sum: Fi = EOD + DPD
    - max: Fi = max(EOD, DPD)

    Parameters
    ----------
    metrics : FairnessMetrics
    mode : str
        'sum' or 'max'
    """
    if mode == "sum":
        return metrics.eod + metrics.dpd
    if mode == "max":
        return max(metrics.eod, metrics.dpd)
    raise ValueError(f"Unknown mode: {mode}")
